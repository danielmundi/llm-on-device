{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael.jarczewski/venv-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-24 15:00:34.079541: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-24 15:00:34.096075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742839234.116129 2808398 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742839234.122238 2808398 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742839234.138788 2808398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742839234.138806 2808398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742839234.138809 2808398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742839234.138810 2808398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-24 15:00:34.146324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TFGPT2LMHeadModel,\n",
    ")\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:00:39.782970: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-03-24 15:00:39.783016: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2025-03-24 15:00:39.783024: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2025-03-24 15:00:39.783032: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-03-24 15:00:39.783039: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: dl-28\n",
      "2025-03-24 15:00:39.783043: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: dl-28\n",
      "2025-03-24 15:00:39.783102: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 535.161.7\n",
      "2025-03-24 15:00:39.783133: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 535.161.7\n",
      "2025-03-24 15:00:39.783137: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 535.161.7\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  124439808 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124439808 (474.70 MB)\n",
      "Trainable params: 124439808 (474.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Lora on GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultar a classe [TFGPT2MainLayer](https://huggingface.co/transformers/v2.0.0/_modules/transformers/modeling_tf_gpt2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraAttn(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Classe que implementa a técnica de LoRA (Low-Rank Adaptation) para camadas de atenção no TensorFlow.\n",
    "\n",
    "        Esta implementação aplica LoRA na camada de atenção do modelo, adaptando sua função de atenção através da introdução de uma perturbação de baixo-rank, representada pelas matrizes `A` e `B`. Essas matrizes são aprendidas durante o treinamento e controlam a adaptação da camada de atenção original.\n",
    "\n",
    "        A classe é projetada para ser utilizada diretamente dentro de uma arquitetura de rede neural baseada em Transformer, como o GPT-2, onde o número de heads de atenção e a dimensionalidade das camadas são bem conhecidos.\n",
    "\n",
    "        A classe realiza as seguintes operações:\n",
    "        1. Inicializa as matrizes `A` e `B` que formam a adaptação de baixo-rank.\n",
    "        2. Aplica uma modulação da camada de atenção original usando essas matrizes.\n",
    "        3. A integração das modificações de LoRA no gráfico computacional do TensorFlow é garantida com a adição de perdas nulas, permitindo que o gradiente seja calculado corretamente durante o treinamento.\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layer: tf.keras.layers.Layer, rank: int, layer_id:int):\n",
    "        super(LoraAttn, self).__init__(name=\"lora_layer\")\n",
    "        self.old_layer = attn_layer\n",
    "        self.old_weights = self.old_layer.get_weights()\n",
    "        self.rank = rank\n",
    "\n",
    "        self.A = self.add_weight(\n",
    "            shape=(768, self.rank), # 768 refere-se à dimensão da entrada de cada cabeça de atenção (no caso do GPT-2)\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name=f\"lora_qa_{layer_id}\"\n",
    "        )\n",
    "\n",
    "        self.B = self.add_weight(\n",
    "            shape=(self.rank, 2304), # 2304 refere-se à dimensão da saída da camada de atenção\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name=f\"lora_qb_{layer_id}\"\n",
    "        )\n",
    "\n",
    "        self.lora_alpha = 1.0\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "            Aplica a modulação LoRA na camada de atenção original.\n",
    "            A camada de atenção original é computada normalmente e depois ajustada com as modificações de baixo-rank introduzidas pelas matrizes `A` e `B`.\n",
    "            inputs: O tensor de entrada para a camada de atenção.\n",
    "            Retorna o Tensor modificado que combina a saída da atenção original com a adaptação de baixo-rank.\n",
    "        \"\"\"\n",
    "        # Passo 1: Obtém a saída da camada de atenção original\n",
    "        original = self.old_layer(inputs)\n",
    "        # Passo 2: Aplica a modulação de baixo-rank LoRA\n",
    "        output = tf.matmul(tf.matmul(inputs, self.A), self.B) * self.lora_alpha\n",
    "        \n",
    "        # Passo 3: Força a inclusão das novas camadas LoRA no grafo computacional do TensorFlow\n",
    "        # Isso é necessário para garantir que o TensorFlow reconheça as operações e calcule gradientes corretamente.\n",
    "        self.add_loss(0.0 * tf.reduce_sum(self.A))\n",
    "        self.add_loss(0.0 * tf.reduce_sum(self.B))\n",
    "        \n",
    "        # Passo 4: Retorna a combinação da saída original com a adaptação LoRA\n",
    "        return original + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros treináveis: 124439808\n"
     ]
    }
   ],
   "source": [
    "trainable_params = model.trainable_variables\n",
    "total_trainable_params = sum([tf.size(variable).numpy() for variable in trainable_params])\n",
    "\n",
    "print(f\"Total de parâmetros treináveis: {total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora():\n",
    "    for i, tfblock in enumerate(model.transformer.h):\n",
    "        original_attn = tfblock.attn.c_attn  # Acessa a camada original de atenção\n",
    "        original_attn.trainable = False  # Congela a camada original de atenção\n",
    "        \n",
    "        # Congela as variáveis da camada original e remove o gradiente\n",
    "        for var in original_attn.variables:\n",
    "            var.assign(tf.stop_gradient(var))  # Remove o gradiente\n",
    "            var._trainable = False  # Congela os pesos da camada original\n",
    "\n",
    "        # Cria a camada LoRA e a integra à camada original de atenção\n",
    "        lora_layer = LoraAttn(original_attn, rank=8, layer_id=i)\n",
    "        for var in lora_layer.variables:\n",
    "            # Apenas os pesos LoRA são treináveis\n",
    "            if \"lora\" in var.name:\n",
    "                var._trainable = True\n",
    "            else:\n",
    "                var._trainable = False  # Congela qualquer outro peso herdado\n",
    "\n",
    "        # Substitui a camada original pela camada LoRA adaptada\n",
    "        setattr(tfblock.attn, \"c_attn\", lora_layer)\n",
    "\n",
    "\n",
    "    # Congela os embeddings e a última camada LayerNorm\n",
    "    model.transformer.wte.trainable = False  # Embeddings de tokens\n",
    "    model.transformer.wpe.trainable = False  # Embeddings de posições\n",
    "    model.transformer.ln_f.trainable = False  # Última LayerNorm\n",
    "\n",
    "    # Congela as camadas de normalização e feedforward do modelo\n",
    "    for i, tfblock in enumerate(model.transformer.h):\n",
    "        tfblock.ln_1.trainable = False  # LayerNorm 1\n",
    "        tfblock.ln_2.trainable = False  # LayerNorm 2\n",
    "        tfblock.attn.c_proj.trainable = False  # Projeção da atenção\n",
    "        tfblock.mlp.c_fc.trainable = False  # Feedforward do MLP\n",
    "        tfblock.mlp.c_proj.trainable = False  # Projeção final do MLP\n",
    "\n",
    "    # Recompila o modelo para registrar as camadas LoRA no gráfico computacional\n",
    "    optimizer = model.optimizer if model.optimizer else tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloco 0 - lora_layer, Treinável: True\n",
      "Bloco 1 - lora_layer, Treinável: True\n",
      "Bloco 2 - lora_layer, Treinável: True\n",
      "Bloco 3 - lora_layer, Treinável: True\n",
      "Bloco 4 - lora_layer, Treinável: True\n",
      "Bloco 5 - lora_layer, Treinável: True\n",
      "Bloco 6 - lora_layer, Treinável: True\n",
      "Bloco 7 - lora_layer, Treinável: True\n",
      "Bloco 8 - lora_layer, Treinável: True\n",
      "Bloco 9 - lora_layer, Treinável: True\n",
      "Bloco 10 - lora_layer, Treinável: True\n",
      "Bloco 11 - lora_layer, Treinável: True\n"
     ]
    }
   ],
   "source": [
    "for i, tfblock in enumerate(model.transformer.h):\n",
    "    print(f\"Bloco {i} - {tfblock.attn.c_attn.name}, Treinável: {tfblock.attn.c_attn.trainable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de Variáveis Treináveis:\n",
      "lora_qa_0:0, Shape: (768, 8)\n",
      "lora_qb_0:0, Shape: (8, 2304)\n",
      "lora_qa_1:0, Shape: (768, 8)\n",
      "lora_qb_1:0, Shape: (8, 2304)\n",
      "lora_qa_2:0, Shape: (768, 8)\n",
      "lora_qb_2:0, Shape: (8, 2304)\n",
      "lora_qa_3:0, Shape: (768, 8)\n",
      "lora_qb_3:0, Shape: (8, 2304)\n",
      "lora_qa_4:0, Shape: (768, 8)\n",
      "lora_qb_4:0, Shape: (8, 2304)\n",
      "lora_qa_5:0, Shape: (768, 8)\n",
      "lora_qb_5:0, Shape: (8, 2304)\n",
      "lora_qa_6:0, Shape: (768, 8)\n",
      "lora_qb_6:0, Shape: (8, 2304)\n",
      "lora_qa_7:0, Shape: (768, 8)\n",
      "lora_qb_7:0, Shape: (8, 2304)\n",
      "lora_qa_8:0, Shape: (768, 8)\n",
      "lora_qb_8:0, Shape: (8, 2304)\n",
      "lora_qa_9:0, Shape: (768, 8)\n",
      "lora_qb_9:0, Shape: (8, 2304)\n",
      "lora_qa_10:0, Shape: (768, 8)\n",
      "lora_qb_10:0, Shape: (8, 2304)\n",
      "lora_qa_11:0, Shape: (768, 8)\n",
      "lora_qb_11:0, Shape: (8, 2304)\n"
     ]
    }
   ],
   "source": [
    "print(\"Lista de Variáveis Treináveis:\")\n",
    "for var in model.trainable_variables:\n",
    "    print(f\"{var.name}, Shape: {var.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros: 124734720\n"
     ]
    }
   ],
   "source": [
    "after_lora_params = model.count_params()\n",
    "print(f\"Total de parâmetros: {after_lora_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora parameters add: 294912\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lora parameters add: {after_lora_params - total_trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros treináveis: 294912\n"
     ]
    }
   ],
   "source": [
    "trainable_params = model.trainable_variables\n",
    "total_trainable_params = sum([tf.size(variable).numpy() for variable in trainable_params])\n",
    "\n",
    "print(f\"Total de parâmetros treináveis: {total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser notado, apenas as camadas do lora estão habilitadas para o treinamento. Certifique-se sempre que as camadas novas do lora foram inseridas corretamente no grafo computacional e que você congelou todas as outras camadas do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversão para .tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Generator(tf.Module):\n",
    "    \"\"\"\n",
    "        Classe para geração de texto baseada no modelo GPT-2, com suporte à adaptação LoRA para ajustar camadas de atenção do modelo sem a necessidade de re-treinamento completo.\n",
    "\n",
    "        Esta classe integra o modelo GPT-2 pré-treinado, e oferece suporte para treinamento e inferência. A adaptação LoRA é aplicada nas camadas de atenção, permitindo um ajuste eficiente do modelo para novos dados ou tarefas específicas com um custo computacional reduzido.\n",
    "\n",
    "        A implementação é otimizada para compatibilidade com LiteRT, permitindo a execução eficiente em dispositivos móveis e incorporados.\n",
    "\n",
    "        Métodos principais:\n",
    "        - **apply_lora()**: Aplica a adaptação LoRA nas camadas de atenção do modelo GPT-2, congelando os pesos das camadas originais e permitindo que apenas os pesos das camadas LoRA sejam treinados.\n",
    "        - **infer()**: Realiza a inferência, ou seja, gera previsões de tokens do modelo com base em entradas fornecidas.\n",
    "        - **train()**: Realiza o treinamento do modelo, aplicando os gradientes apenas aos pesos LoRA treináveis e utilizando o otimizador AdamW.\n",
    "        - **get_w()**: Retorna os pesos do modelo, útil para a análise e monitoramento dos parâmetros do modelo durante o treinamento.\n",
    "\n",
    "        A classe permite personalização da aplicação LoRA nas camadas de atenção e configuração da taxa de aprendizado.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = tf.keras.optimizers.AdamW()\n",
    "    ## Algumas funções podem ser importantes durante o processo de geração, como nem todas as operações são compativeis durante a conversão, o decorator abaixo ajuda a identificar esse problemas de incompatibilidade e ira mostrar um warning ou compatibility error\n",
    "    ## Para mais informações consulte https://ai.google.dev/edge/litert/models/authoring\n",
    "    @tf.lite.experimental.authoring.compatible\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec([None, None], tf.int32),\n",
    "            tf.TensorSpec([None, None], tf.int32)\n",
    "        ]\n",
    "    )\n",
    "    def infer(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "            Realiza a inferência do modelo para gerar a próxima previsão de tokens.\n",
    "\n",
    "            Args:\n",
    "                input_ids (tf.Tensor): IDs de entrada dos tokens.\n",
    "                attention_mask (tf.Tensor): Máscara de atenção para a sequência de entrada.\n",
    "\n",
    "            Returns:\n",
    "                dict: Contém os logits da previsão de tokens.\n",
    "        \"\"\"\n",
    "        # Realiza a previsão, retornando a última previsão de token\n",
    "        tokens = self.model(input_ids=input_ids, attention_mask=attention_mask, training=False).logits[0:, -1]\n",
    "        return {\n",
    "            \"logits\": tokens\n",
    "        }\n",
    "\n",
    "    @tf.lite.experimental.authoring.compatible\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec([None, None], tf.int32),\n",
    "            tf.TensorSpec([None, None], tf.int32),\n",
    "            tf.TensorSpec([None, None], tf.int32),\n",
    "        ]\n",
    "    )\n",
    "    def train(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"\n",
    "            Função de treinamento para aplicar gradientes aos pesos LoRA treináveis.\n",
    "            A função realiza um ciclo de feedforward e backward apenas nos pesos LoRA treináveis.\n",
    "            Args:\n",
    "                input_ids (tf.Tensor): IDs dos tokens de entrada.\n",
    "                attention_mask (tf.Tensor): Máscara de atenção.\n",
    "                labels (tf.Tensor): Labels para treinamento supervisionado.\n",
    "            Returns:\n",
    "                dict: Contém a perda calculada durante o treinamento.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                training=True\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "        # Aplica os gradientes aos pesos LoRA treináveis\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @tf.lite.experimental.authoring.compatible\n",
    "    @tf.function\n",
    "    def get_w(self):\n",
    "        weights = {}\n",
    "        for var in self.model.weights:\n",
    "            weights[var.name] = var\n",
    "        return weights\n",
    "\n",
    "    @tf.lite.experimental.authoring.compatible\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "    def save(self, checkpoint_path):\n",
    "        tensor_names = [weight.name for weight in self.model.weights if 'lora' in weight.name]\n",
    "        tensors_to_save = [weight.read_value() for weight in self.model.weights if 'lora' in weight.name]\n",
    "        tf.raw_ops.Save(\n",
    "            filename=checkpoint_path,\n",
    "            tensor_names=tensor_names,\n",
    "            data=tensors_to_save,\n",
    "            name='save'\n",
    "        )\n",
    "        return {\n",
    "            \"checkpoint_path\": checkpoint_path\n",
    "        }\n",
    "\n",
    "    @tf.lite.experimental.authoring.compatible\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "    def restore(self, checkpoint_path):\n",
    "        restored_tensors = {}\n",
    "        for var in self.model.weights:\n",
    "            if 'lora' in var.name:\n",
    "                restored = tf.raw_ops.Restore(\n",
    "                    file_pattern=checkpoint_path,\n",
    "                    tensor_name=var.name,\n",
    "                    dt=var.dtype,\n",
    "                    name='restore'\n",
    "                )\n",
    "                var.assign(restored)\n",
    "                restored_tensors[var.name] = restored\n",
    "        return {\"resposta\": \"restored\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"lora_model.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2converter = GPT2Generator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos criar um dicionário com as funções concretas das assinaturas para integrar ao saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = {\n",
    "    \"train\": model2converter.train.get_concrete_function(),\n",
    "    \"infer\": model2converter.infer.get_concrete_function(),\n",
    "    \"get_parameters\": model2converter.get_w.get_concrete_function(),\n",
    "    \"save\": model2converter.save.get_concrete_function(),\n",
    "    \"restore\": model2converter.restore.get_concrete_function(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos salvar o nosso modelo através da função saved_model do tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(\n",
    "    obj = model2converter,\n",
    "    export_dir = saved_model_path,\n",
    "    signatures = signatures,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como utilizamos o saved model do tensorflow conseguimos criar um converter através do diretório que armazena as informações do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_72045) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_72192) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para garantir que as operações que vamos criar terão compatibilidade durante a execução no dispositivos precisamos informar quais conjuntos de operações que nosso modelo utiliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, # Habilita as ops LiteRT.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS, # Habilita as ops TensorFlow.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilita resource variables que adiciona garantias de leitura e gravação mais fortes (Opcional)\n",
    "converter.experimental_enable_resource_variables = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert() # Conversão de fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
