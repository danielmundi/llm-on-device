{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozymandias/llms/vteste/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-12 08:59:39.651808: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-12 08:59:39.854854: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-12 08:59:40.035730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741780780.185475    7109 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741780780.228141    7109 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 08:59:40.598539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TFGPT2LMHeadModel,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741780785.314723    7109 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  124439808 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124439808 (474.70 MB)\n",
      "Trainable params: 124439808 (474.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(array):\n",
    "    return tf.reshape(\n",
    "        tf.convert_to_tensor(array),\n",
    "        (-1, 32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn1 = tokenizer(\"how are\", return_tensors='tf')\n",
    "tkn2 = tokenizer(\"What is your\", return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tkn1['input_ids']\n",
    "# input_ids = tf.convert_to_tensor([tkn1['input_ids'], tkn2['input_ids']])\n",
    "# input_ids = padding([tokens['input_ids'][0][x] if x < tokens['input_ids'][0].shape[0] else 0 for x in range(32)])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = tkn1['attention_mask']\n",
    "# attention_mask = tf.convert_to_tensor([tkn1['attention_mask'], tkn2['attention_mask']])\n",
    "# attention_mask = padding([1 if x < tokens['attention_mask'][0].shape[0] else 0 for x in range(32)])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model([input_ids]).logits[0:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.math.argmax(logits[-1:,], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Lora on GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultar a classe [TFGPT2MainLayer](https://huggingface.co/transformers/v2.0.0/_modules/transformers/modeling_tf_gpt2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LoRALayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, attn_layer: tf.keras.layers.Layer, rank: int):\n",
    "#         super(LoRALayer, self).__init__()\n",
    "#         self.old_layer = attn_layer\n",
    "#         self.rank = rank\n",
    "\n",
    "#         self.A_q = self.add_weight(\n",
    "#             shape=(self.old_layer.n_head, 1, self.rank),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name=\"lora_qa\")\n",
    "\n",
    "#         self.B_q = self.add_weight(\n",
    "#             shape=(1, self.rank, 64),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name=\"lora_qb\")\n",
    "        \n",
    "#         self.A_k = self.add_weight(\n",
    "#             shape=(self.old_layer.n_head, 1, self.rank),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name=\"lora_ka\")\n",
    "\n",
    "#         self.B_k = self.add_weight(\n",
    "#             shape=(1, self.rank, 64),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name=\"lora_kb\")\n",
    "\n",
    "#         self.A_v = self.add_weight(\n",
    "#             shape=(self.old_layer.n_head, 1, self.rank),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name=\"lora_va\")\n",
    "\n",
    "#         self.B_v = self.add_weight(\n",
    "#             shape=(1, self.rank, 64),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name=\"lora_vb\")\n",
    "\n",
    "#         self.lora_alpha = 1.0\n",
    "\n",
    "#     def call(self, inputs,\n",
    "#         training=False,\n",
    "#         layer_past=None,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         encoder_hidden_states=None,\n",
    "#         encoder_attention_mask=None,\n",
    "#         use_cache=None,\n",
    "#         output_attentions=None,\n",
    "#     ):\n",
    "#         x = inputs\n",
    "#         x = self.old_layer.c_attn(x)\n",
    "#         query, key, value = tf.split(x, 3, axis=2)\n",
    "#         query = self.old_layer.split_heads(query)\n",
    "#         key = self.old_layer.split_heads(key)\n",
    "#         value = self.old_layer.split_heads(value)\n",
    "        \n",
    "#         if layer_past is not None:\n",
    "#             past_key, past_value = tf.unstack(layer_past, axis=1)\n",
    "#             key = tf.concat([past_key, key], axis=-2)\n",
    "#             value = tf.concat([past_value, value], axis=-2)\n",
    "#         present = tf.stack([key, value], axis=1)\n",
    "        \n",
    "#         AB_q = tf.matmul(self.A_q, self.B_q)\n",
    "        \n",
    "#         AB_q = tf.reshape(AB_q, shape=(query.shape[0],) + AB_q.shape[0:])\n",
    "#         AB_k = tf.matmul(self.A_k, self.B_k)\n",
    "#         AB_k = tf.reshape(AB_k, shape=(key.shape[0],) + AB_k.shape[0:])\n",
    "#         AB_v = tf.matmul(self.A_v, self.B_v)\n",
    "#         AB_v = tf.reshape(AB_v, shape=(value.shape[0],) + AB_v.shape[0:])\n",
    "        \n",
    "#         query = query + 1 * AB_q\n",
    "#         key = key + 1 * AB_k\n",
    "#         value = value + 1 * AB_v\n",
    "        \n",
    "#         attn_outputs = self.old_layer._attn(q=query,k=key, v=value, attention_mask=attention_mask, head_mask=head_mask, training=training, output_attentions=output_attentions)\n",
    "#         a = attn_outputs[0]\n",
    "\n",
    "#         a = self.old_layer.merge_heads(a)\n",
    "#         a = self.old_layer.c_proj(a)\n",
    "#         a = self.old_layer.resid_dropout(a, training=training)\n",
    "\n",
    "#         outputs = [a, present] + attn_outputs[1:]\n",
    "#         return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificando as camadas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos aplicar o LoRA nas camadas do TFGPT2MainLayer bem como congelá-las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUM LAYERS: {model.transformer.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraAttn(tf.keras.layers.Layer):\n",
    "    def __init__(self, attn_layer: tf.keras.layers.Layer, rank: int, layer_id:int):\n",
    "        super(LoraAttn, self).__init__(name=\"lora_layer\")\n",
    "        self.old_layer = attn_layer\n",
    "        self.old_weights = self.old_layer.get_weights()\n",
    "        self.rank = rank\n",
    "\n",
    "        self.A = self.add_weight(\n",
    "            shape=(768, self.rank),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name=f\"lora_qa_{layer_id}\"\n",
    "        )\n",
    "\n",
    "        self.B = self.add_weight(\n",
    "            shape=(self.rank, 2304),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name=f\"lora_qb_{layer_id}\"\n",
    "        )\n",
    "\n",
    "        self.lora_alpha = 1.0\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        original = self.old_layer(inputs)\n",
    "\n",
    "        output = tf.matmul(tf.matmul(inputs, self.A), self.B) * self.lora_alpha\n",
    "        \n",
    "        self.add_loss(0.0 * tf.reduce_sum(self.A))  \n",
    "        self.add_loss(0.0 * tf.reduce_sum(self.B))\n",
    "\n",
    "        return original + output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como identificado precisamos alterar a variável 'h' que possuem a estrutura do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo possuem 12 camadas TFBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.mlp\n",
    "# Para congelar\n",
    "layer.mlp.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768, 3072])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.mlp.c_fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 768)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = tf.random.uniform(shape=(1, 10, 768))  # 1 exemplo, 10 tokens, 768 features\n",
    "\n",
    "output = layer(\n",
    "    dummy_input,\n",
    "    layer_past=None,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None\n",
    ")\n",
    "print(output[0].shape)  # Esperado: (1, 10, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.mlp.c_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = layer.attn(tf.random.uniform(shape=(1, 10, 768)),  layer_past=None, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, use_cache=None, output_attentions=None)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.attn.weights[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.attn.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.attn.c_proj.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada TFBlock possue 2 Layer Normalization (ln_1 e ln_2) e uma multi layer perceptron (mlp) do tipo TFMLP e um attention (TFAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "# model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  124439808 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124439808 (474.70 MB)\n",
      "Trainable params: 124439808 (474.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros treináveis: 124439808\n"
     ]
    }
   ],
   "source": [
    "trainable_params = model.trainable_variables\n",
    "total_trainable_params = sum([tf.size(variable).numpy() for variable in trainable_params])\n",
    "\n",
    "print(f\"Total de parâmetros treináveis: {total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros: 124439808\n"
     ]
    }
   ],
   "source": [
    "before_lora_params = model.count_params()\n",
    "print(f\"Total de parâmetros: {before_lora_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tfblock in enumerate(model.transformer.h):\n",
    "    original_attn = tfblock.attn.c_attn\n",
    "    original_attn.trainable = False\n",
    "    for var in original_attn.variables:\n",
    "        var._trainable = False  # Congelamento manual\n",
    "    setattr(tfblock.attn, \"c_attn\", LoraAttn(original_attn, rank=8, layer_id=i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloco 0 - lora_layer, Treinável: True\n",
      "Bloco 1 - lora_layer, Treinável: True\n",
      "Bloco 2 - lora_layer, Treinável: True\n",
      "Bloco 3 - lora_layer, Treinável: True\n",
      "Bloco 4 - lora_layer, Treinável: True\n",
      "Bloco 5 - lora_layer, Treinável: True\n",
      "Bloco 6 - lora_layer, Treinável: True\n",
      "Bloco 7 - lora_layer, Treinável: True\n",
      "Bloco 8 - lora_layer, Treinável: True\n",
      "Bloco 9 - lora_layer, Treinável: True\n",
      "Bloco 10 - lora_layer, Treinável: True\n",
      "Bloco 11 - lora_layer, Treinável: True\n"
     ]
    }
   ],
   "source": [
    "for i, tfblock in enumerate(model.transformer.h):\n",
    "    print(f\"Bloco {i} - {tfblock.attn.c_attn.name}, Treinável: {tfblock.attn.c_attn.trainable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Lista de Variáveis Treináveis:\n",
      "tfgpt2lm_head_model_19/transformer/wte/embeddings:0, Shape: (50257, 768)\n",
      "tfgpt2lm_head_model_19/transformer/wpe/embeddings:0, Shape: (1024, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_0:0, Shape: (768, 8)\n",
      "lora_qb_0:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._0/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_1:0, Shape: (768, 8)\n",
      "lora_qb_1:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._1/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_2:0, Shape: (768, 8)\n",
      "lora_qb_2:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._2/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_3:0, Shape: (768, 8)\n",
      "lora_qb_3:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._3/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_4:0, Shape: (768, 8)\n",
      "lora_qb_4:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._4/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_5:0, Shape: (768, 8)\n",
      "lora_qb_5:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._5/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_6:0, Shape: (768, 8)\n",
      "lora_qb_6:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._6/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_7:0, Shape: (768, 8)\n",
      "lora_qb_7:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._7/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_8:0, Shape: (768, 8)\n",
      "lora_qb_8:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._8/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_9:0, Shape: (768, 8)\n",
      "lora_qb_9:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._9/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_10:0, Shape: (768, 8)\n",
      "lora_qb_10:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._10/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/ln_1/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/ln_1/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/attn/c_proj/weight:0, Shape: (768, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/attn/c_proj/bias:0, Shape: (1, 768)\n",
      "lora_qa_11:0, Shape: (768, 8)\n",
      "lora_qb_11:0, Shape: (8, 2304)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/ln_2/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/ln_2/beta:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/mlp/c_fc/weight:0, Shape: (768, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/mlp/c_fc/bias:0, Shape: (1, 3072)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/mlp/c_proj/weight:0, Shape: (3072, 768)\n",
      "tfgpt2lm_head_model_19/transformer/h_._11/mlp/c_proj/bias:0, Shape: (1, 768)\n",
      "tfgpt2lm_head_model_19/transformer/ln_f/gamma:0, Shape: (768,)\n",
      "tfgpt2lm_head_model_19/transformer/ln_f/beta:0, Shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Lista de Variáveis Treináveis:\")\n",
    "for var in model.trainable_variables:\n",
    "    print(f\"{var.name}, Shape: {var.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos recompilar o modelo porque se não tensorflow não considera ele no grafo computacional, bem como não podemos simplismente utilizar trainable = False e depois aplicar o lora, pois todos pesos são congelados recursivamente. A solução que encontrei foi congelar individualmente cada camada e garantir que o Lora não esteja congelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_qa_0:0, Shape: (768, 8)\n",
      "lora_qb_0:0, Shape: (8, 2304)\n",
      "lora_qa_1:0, Shape: (768, 8)\n",
      "lora_qb_1:0, Shape: (8, 2304)\n",
      "lora_qa_2:0, Shape: (768, 8)\n",
      "lora_qb_2:0, Shape: (8, 2304)\n",
      "lora_qa_3:0, Shape: (768, 8)\n",
      "lora_qb_3:0, Shape: (8, 2304)\n",
      "lora_qa_4:0, Shape: (768, 8)\n",
      "lora_qb_4:0, Shape: (8, 2304)\n",
      "lora_qa_5:0, Shape: (768, 8)\n",
      "lora_qb_5:0, Shape: (8, 2304)\n",
      "lora_qa_6:0, Shape: (768, 8)\n",
      "lora_qb_6:0, Shape: (8, 2304)\n",
      "lora_qa_7:0, Shape: (768, 8)\n",
      "lora_qb_7:0, Shape: (8, 2304)\n",
      "lora_qa_8:0, Shape: (768, 8)\n",
      "lora_qb_8:0, Shape: (8, 2304)\n",
      "lora_qa_9:0, Shape: (768, 8)\n",
      "lora_qb_9:0, Shape: (8, 2304)\n",
      "lora_qa_10:0, Shape: (768, 8)\n",
      "lora_qb_10:0, Shape: (8, 2304)\n",
      "lora_qa_11:0, Shape: (768, 8)\n",
      "lora_qb_11:0, Shape: (8, 2304)\n"
     ]
    }
   ],
   "source": [
    "for i, tfblock in enumerate(model.transformer.h):\n",
    "    original_attn = tfblock.attn.c_attn  # Pegamos a camada original\n",
    "    original_attn.trainable = False  \n",
    "    for var in original_attn.variables:\n",
    "        var.assign(tf.stop_gradient(var))  # Remove gradiente\n",
    "        var._trainable = False  # Congela os pesos antigos\n",
    "    lora_layer = LoraAttn(original_attn, rank=8, layer_id=i)\n",
    "    for var in lora_layer.variables:\n",
    "        if \"lora\" in var.name:  # Somente os pesos LoRA podem ser treináveis\n",
    "            var._trainable = True  \n",
    "        else:\n",
    "            var._trainable = False  # Congela qualquer outro peso herdado\n",
    "    setattr(tfblock.attn, \"c_attn\", lora_layer)\n",
    "model.transformer.wte.trainable = False  # Embeddings\n",
    "model.transformer.wpe.trainable = False  # Embeddings\n",
    "model.transformer.ln_f.trainable = False  # Última LayerNorm\n",
    "\n",
    "for i, tfblock in enumerate(model.transformer.h):\n",
    "    tfblock.ln_1.trainable = False  # LayerNorm 1\n",
    "    tfblock.ln_2.trainable = False  # LayerNorm 2\n",
    "    tfblock.attn.c_proj.trainable = False  # Projeção da atenção\n",
    "    tfblock.mlp.c_fc.trainable = False  # Feedforward do MLP\n",
    "    tfblock.mlp.c_proj.trainable = False  # Projeção final do MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Lista Final de Variáveis Treináveis:\n",
      "lora_qa_0:0, Shape: (768, 8)\n",
      "lora_qb_0:0, Shape: (8, 2304)\n",
      "lora_qa_1:0, Shape: (768, 8)\n",
      "lora_qb_1:0, Shape: (8, 2304)\n",
      "lora_qa_2:0, Shape: (768, 8)\n",
      "lora_qb_2:0, Shape: (8, 2304)\n",
      "lora_qa_3:0, Shape: (768, 8)\n",
      "lora_qb_3:0, Shape: (8, 2304)\n",
      "lora_qa_4:0, Shape: (768, 8)\n",
      "lora_qb_4:0, Shape: (8, 2304)\n",
      "lora_qa_5:0, Shape: (768, 8)\n",
      "lora_qb_5:0, Shape: (8, 2304)\n",
      "lora_qa_6:0, Shape: (768, 8)\n",
      "lora_qb_6:0, Shape: (8, 2304)\n",
      "lora_qa_7:0, Shape: (768, 8)\n",
      "lora_qb_7:0, Shape: (8, 2304)\n",
      "lora_qa_8:0, Shape: (768, 8)\n",
      "lora_qb_8:0, Shape: (8, 2304)\n",
      "lora_qa_9:0, Shape: (768, 8)\n",
      "lora_qb_9:0, Shape: (8, 2304)\n",
      "lora_qa_10:0, Shape: (768, 8)\n",
      "lora_qb_10:0, Shape: (8, 2304)\n",
      "lora_qa_11:0, Shape: (768, 8)\n",
      "lora_qb_11:0, Shape: (8, 2304)\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.optimizer if model.optimizer else tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer)\n",
    "for var in model.trainable_variables:\n",
    "    print(f\"{var.name}, Shape: {var.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros: 124734720\n"
     ]
    }
   ],
   "source": [
    "after_lora_params = model.count_params()\n",
    "print(f\"Total de parâmetros: {after_lora_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora parameters add: 294912\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lora parameters add: {after_lora_params - before_lora_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros treináveis: 294912\n"
     ]
    }
   ],
   "source": [
    "trainable_params = model.trainable_variables\n",
    "total_trainable_params = sum([tf.size(variable).numpy() for variable in trainable_params])\n",
    "\n",
    "print(f\"Total de parâmetros treináveis: {total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos treináveis:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPesos treináveis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrainable_variables:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(var\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m      5\u001b[0m total_params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcount_params()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Pesos treináveis:\")\n",
    "for var in model.trainable_variables:\n",
    "    print(var.name)\n",
    "\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum(tf.size(v).numpy() for v in model.trainable_variables)\n",
    "\n",
    "print(f\"TOTAL DE PARÂMETROS: {total_params}\")\n",
    "print(f\"SÓ LO-RA TREINÁVEL: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"How are\", return_tensors='tf')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#1, 12, 3, 64\n",
    "rank=58\n",
    "matA = np.zeros(shape=(1, 10, rank))\n",
    "matB = np.zeros(shape=(1, rank, 798))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 798)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(matA, matB).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(shape=(1, 4, 768))\n",
    "tf.split(a, 3, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[6.1173854,\n",
    "5.804854,\n",
    "6.236913,\n",
    "7.7318535,\n",
    "6.704432,\n",
    "5.7643027,\n",
    "6.005016,\n",
    "5.6441536,\n",
    "6.3495393,\n",
    "5.589044,\n",
    "6.232273,\n",
    "6.3398476,\n",
    "5.7454095,\n",
    "7.5725613,\n",
    "6.3798594,\n",
    "6.8409615,\n",
    "6.9971232,\n",
    "5.8408737,\n",
    "6.5225673,\n",
    "6.8489685,\n",
    "6.955326,\n",
    "6.2124715,\n",
    "6.583052,\n",
    "6.4961452,\n",
    "6.83617,\n",
    "6.758698,\n",
    "6.2365794,\n",
    "5.210693,\n",
    "6.3284087,\n",
    "5.6087747,\n",
    "6.641953,\n",
    "7.1969275,\n",
    "5.9185653,\n",
    "5.8820357,\n",
    "5.792418,\n",
    "5.6274476,\n",
    "7.6170025,\n",
    "5.5744443,\n",
    "6.745022,\n",
    "6.573751,\n",
    "6.9048324,\n",
    "5.7228107,\n",
    "5.795529,\n",
    "6.276531,\n",
    "5.8931737,\n",
    "5.3276315,\n",
    "5.7939715,\n",
    "5.947024,\n",
    "5.6362395,\n",
    "6.774073,\n",
    "5.738224,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=list(range(len(a))), y=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vteste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
